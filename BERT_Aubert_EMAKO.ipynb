{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUgsT6-mE6-i"
   },
   "source": [
    "#Deep learning challenge using BERT fine Tuning sentence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivRGWu3G394m"
   },
   "source": [
    "## Import and installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJQSsZIU3Wgf"
   },
   "outputs": [],
   "source": [
    "# Install Kaggle library\n",
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "colab_type": "code",
    "id": "q9Of7Lvg4Szt",
    "outputId": "2d341f1b-167b-4458-a2f1-56efffbe5954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
      "\r",
      "\u001b[K     |▊                               | 10kB 20.5MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 20kB 26.3MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 30kB 28.9MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 40kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 51kB 33.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 61kB 36.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 71kB 34.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 81kB 35.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 92kB 31.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 102kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 112kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 122kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 133kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 143kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 153kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 163kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 174kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 184kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 194kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 204kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 215kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 225kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 235kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 245kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 256kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 266kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 276kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 286kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 296kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 307kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 317kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 327kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 337kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 348kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 358kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 368kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 378kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 389kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 399kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 409kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 419kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 430kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 440kB 31.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 450kB 31.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\r",
      "\u001b[K     |▍                               | 10kB 22.0MB/s eta 0:00:01\r",
      "\u001b[K     |▊                               | 20kB 30.3MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 30kB 37.1MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 40kB 29.1MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 51kB 32.4MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 61kB 36.1MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 71kB 38.9MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 81kB 40.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 92kB 41.6MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 102kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 112kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 122kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 133kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 143kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 153kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 163kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 174kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 184kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 194kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 204kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 215kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 225kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 235kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 245kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 256kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 266kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 276kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 286kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 296kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 307kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 317kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 327kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 337kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 348kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 358kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 368kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 378kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 389kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 399kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 409kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 419kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 430kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 440kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 450kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 460kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 471kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 481kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 491kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 501kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 512kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 522kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 532kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 542kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 552kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 563kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 573kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 583kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 593kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 604kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 614kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 624kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 634kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 645kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 655kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 665kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 675kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 686kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 696kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 706kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▋     | 716kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 727kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 737kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 747kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 757kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 768kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 778kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 788kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 798kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 808kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 819kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 829kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 839kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 849kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 860kB 42.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 870kB 42.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 49.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=e25823dbfbd7e14eb7e2cd74fec6f4544f5601971307794204000c3a4511d98a\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, sentencepiece, transformers\n",
      "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n"
     ]
    }
   ],
   "source": [
    "# Install transformers library from huggingface\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "AhEtezre4EVE",
    "outputId": "d416d33b-b5d0-427c-f251-55f3440f340a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Colab library to upload files to notebook\n",
    "from google.colab import files\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "colab_type": "code",
    "id": "yXOu3k2O316L",
    "outputId": "a6c77430-d60a-4f3c-d131-a2f5fb7962db"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-9c86ab04-8878-42b8-89a2-c50d64bf0a26\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-9c86ab04-8878-42b8-89a2-c50d64bf0a26\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kaggle.json to kaggle.json\n"
     ]
    }
   ],
   "source": [
    "uploaded = files.upload() #Upload kaggle.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "kc_STc8P6adW",
    "outputId": "c9c36de7-f737-4762-8724-a210086071bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
      "Downloading example_submission_test.csv to /content\n",
      "  0% 0.00/315k [00:00<?, ?B/s]\n",
      "100% 315k/315k [00:00<00:00, 47.0MB/s]\n",
      "Downloading dataset_train.csv.zip to /content\n",
      " 33% 9.00M/27.4M [00:00<00:01, 10.9MB/s]\n",
      "100% 27.4M/27.4M [00:00<00:00, 35.2MB/s]\n",
      "Downloading dataset_test_no_labels.csv.zip to /content\n",
      "  0% 0.00/1.34M [00:00<?, ?B/s]\n",
      "100% 1.34M/1.34M [00:00<00:00, 89.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!kaggle competitions download -c efreiparisdeeplearning2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZERjCOH5idE"
   },
   "outputs": [],
   "source": [
    "# Import the test and train datasets into pandas dataframe\n",
    "df_train = pd.read_csv('dataset_train.csv.zip', compression='zip', sep='\\t')\n",
    "df_test = pd.read_csv('dataset_test_no_labels.csv.zip', compression='zip', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T09R64DdEMfd"
   },
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "l0u8GlZU5ifD",
    "outputId": "a1df2c1c-a504-41e5-db66-f30de54c2bcd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  ...       label\n",
       "0      0  ...     neutral\n",
       "1      1  ...  entailment\n",
       "2      2  ...  entailment\n",
       "3      3  ...  entailment\n",
       "4      4  ...     neutral\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8eu_TLn55ik3",
    "outputId": "9067a4cf-1f0e-437f-96cb-b88a770ce8db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'sentence_1', 'sentence_2', 'label'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "L_8semJy3WyD",
    "outputId": "84230e85-1415-4082-bb3e-9ca5fde5f0ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 1\n",
      "sentence_1 you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him\n",
      "sentence_2 You lose the things to the following level if the people recall.\n",
      "label entailment\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for x in df_train.columns:\n",
    "  print(x, df_train[x][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "MSiUKEl63W0R",
    "outputId": "88b9fac3-c2b9-4878-e9d3-1ee09594d23d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'entailment', 'contradiction'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "0TfHzd7K3W2W",
    "outputId": "790e9f76-46fd-4ede-c996-795fa28d25e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "contradiction    130889\n",
       "entailment       130886\n",
       "neutral          130887\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby(['label'])['label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HESk_l4pA_61",
    "outputId": "9f738c7c-28a5-4f05-fa42-08fb97a64d1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df_train.label = label_encoder.fit_transform(df_train.label)\n",
    "\n",
    "df_train.label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUF5kfUB0iI5"
   },
   "outputs": [],
   "source": [
    "sentences_1 = df_train.sentence_1.values\n",
    "sentences_2 = df_train.sentence_2.values\n",
    "labels = df_train.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HUOl4CP0iPg"
   },
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "rjHUtJkS0iid",
    "outputId": "ac12c1cc-0559-44ee-ca41-7801a907ca7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Conceptually cream skimming has two basic dimensions - product and geography.\n",
      "Tokenized:  ['conceptual', '##ly', 'cream', 'ski', '##mming', 'has', 'two', 'basic', 'dimensions', '-', 'product', 'and', 'geography', '.']\n",
      "Token IDs:  [17158, 2135, 6949, 8301, 25057, 2038, 2048, 3937, 9646, 1011, 4031, 1998, 10505, 1012]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', sentences_1[0])\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences_1[0]))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences_1[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ofDEICiE0ilK",
    "outputId": "e0482237-f4d7-4f8b-a6b5-d8998f33298c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Conceptually cream skimming has two basic dimensions - product and geography.\n",
      "Token IDs: [101, 17158, 2135, 6949, 8301, 25057, 2038, 2048, 3937, 9646, 1011, 4031, 1998, 10505, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "input_ids_1 = []\n",
    "MAX_LEN = 64\n",
    "\n",
    "for sent in sentences_1:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]', CLS to the start and SEP to the end \n",
    "                        max_length = MAX_LEN,      # Truncate all sentences to 64.\n",
    "                   )\n",
    "    input_ids_1.append(encoded_sent)\n",
    "\n",
    "print('Original: ', sentences_1[0])\n",
    "print('Token IDs:', input_ids_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GCDMQyrr0ivz"
   },
   "outputs": [],
   "source": [
    "# Pad and truncate at the end of the sequence\n",
    "input_ids_1 = pad_sequences(input_ids_1, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "print('Padded Token IDs:', input_ids_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "he40luoBY0hN",
    "outputId": "1d2bb9e8-8727-4d33-8d74-64af8871d7d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Product and geography are what make cream skimming work. \n",
      "Token IDs: [101, 4031, 1998, 10505, 2024, 2054, 2191, 6949, 8301, 25057, 2147, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "# Do the same with the second sentence\n",
    "input_ids_2 = []\n",
    "\n",
    "for sent in sentences_2:\n",
    "\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]', CLS to the start and SEP to the end \n",
    "                        max_length = MAX_LEN,      # Truncate all sentences to 64.\n",
    "                   )\n",
    "    input_ids_2.append(encoded_sent)\n",
    "\n",
    "print('Original: ', sentences_2[0])\n",
    "print('Token IDs:', input_ids_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "geR_t1q4Y0wA",
    "outputId": "8b555ff2-a7af-4890-9ecf-36c6f3c18877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Token IDs: [  101  4031  1998 10505  2024  2054  2191  6949  8301 25057  2147  1012\n",
      "   102     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Pad and truncate at the end of the sequence\n",
    "input_ids_2 = pad_sequences(input_ids_2, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "print('Padded Token IDs:', input_ids_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "H1-BHhUrdN9y",
    "outputId": "f751aadc-566d-41fa-844e-fb45f4d2d3b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our input: [  101 17158  2135  6949  8301 25057  2038  2048  3937  9646  1011  4031\n",
      "  1998 10505  1012   102     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0   101  4031  1998 10505  2024  2054  2191  6949\n",
      "  8301 25057  2147  1012   102     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "#Concatenate our two padded tokenized sentence in order to have only one input for our model\n",
    "input_ids = np.concatenate((input_ids_1, input_ids_2), axis=1)\n",
    "print('Our input:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fM7zstGT0iuc"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent] # Set 0 for padding token and 1 for real token\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Mk5O6te0itB"
   },
   "outputs": [],
   "source": [
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=42, test_size=0.1)\n",
    "# Same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
    "                                             random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "op7xIXKN0ir6"
   },
   "outputs": [],
   "source": [
    "# Convert all inputs and labels into torch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CfKzZIAY0iNR"
   },
   "outputs": [],
   "source": [
    "# DataLoader \n",
    "batch_size = 32\n",
    "\n",
    "# DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hexSjmKlEqqH"
   },
   "source": [
    "## Training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "VvxswHBgBP7O",
    "outputId": "d052df61-8eb1-4b1b-afbe-b8db323c71b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Use GPU instead of CPU if we have one available\n",
    "if torch.cuda.is_available():      \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('%d GPU available.' % torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPU available')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "poYctM8dRpkX",
    "outputId": "f51fbd6c-6d7b-46a6-bc74-daca43cbaaba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification for sentence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",          # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 3,               # The number of output labels  \n",
    "    output_attentions = False,    # Not returns attentions weights.\n",
    "    output_hidden_states = False, # Not returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Run the model on the GPU.\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 653
    },
    "colab_type": "code",
    "id": "zoVidjzWRprL",
    "outputId": "40b723d1-101a-4afe-dbb9-714d4f1b9a36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (3, 768)\n",
      "classifier.bias                                                 (3,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3cTLzXERp5K"
   },
   "outputs": [],
   "source": [
    "# Use AdamW, the class from the huggingface library \n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate\n",
    "                  eps = 1e-8 # default value\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rrNNwFjRp-g"
   },
   "outputs": [],
   "source": [
    "# Training epochs \n",
    "epochs = 2\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RgT_sXt4RqCD"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "znJEr8F8SzAd"
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Djq3HBqkSzKQ",
    "outputId": "10a67dde-2527-4a0f-d3c7-b74dbb1d0fc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of  11,044.    Elapsed: 0:00:18.\n",
      "  Batch    80  of  11,044.    Elapsed: 0:00:36.\n",
      "  Batch   120  of  11,044.    Elapsed: 0:00:54.\n",
      "  Batch   160  of  11,044.    Elapsed: 0:01:12.\n",
      "  Batch   200  of  11,044.    Elapsed: 0:01:30.\n",
      "  Batch   240  of  11,044.    Elapsed: 0:01:48.\n",
      "  Batch   280  of  11,044.    Elapsed: 0:02:06.\n",
      "  Batch   320  of  11,044.    Elapsed: 0:02:24.\n",
      "  Batch   360  of  11,044.    Elapsed: 0:02:42.\n",
      "  Batch   400  of  11,044.    Elapsed: 0:03:00.\n",
      "  Batch   440  of  11,044.    Elapsed: 0:03:18.\n",
      "  Batch   480  of  11,044.    Elapsed: 0:03:36.\n",
      "  Batch   520  of  11,044.    Elapsed: 0:03:54.\n",
      "  Batch   560  of  11,044.    Elapsed: 0:04:12.\n",
      "  Batch   600  of  11,044.    Elapsed: 0:04:30.\n",
      "  Batch   640  of  11,044.    Elapsed: 0:04:48.\n",
      "  Batch   680  of  11,044.    Elapsed: 0:05:06.\n",
      "  Batch   720  of  11,044.    Elapsed: 0:05:24.\n",
      "  Batch   760  of  11,044.    Elapsed: 0:05:42.\n",
      "  Batch   800  of  11,044.    Elapsed: 0:06:00.\n",
      "  Batch   840  of  11,044.    Elapsed: 0:06:18.\n",
      "  Batch   880  of  11,044.    Elapsed: 0:06:36.\n",
      "  Batch   920  of  11,044.    Elapsed: 0:06:54.\n",
      "  Batch   960  of  11,044.    Elapsed: 0:07:12.\n",
      "  Batch 1,000  of  11,044.    Elapsed: 0:07:30.\n",
      "  Batch 1,040  of  11,044.    Elapsed: 0:07:48.\n",
      "  Batch 1,080  of  11,044.    Elapsed: 0:08:06.\n",
      "  Batch 1,120  of  11,044.    Elapsed: 0:08:24.\n",
      "  Batch 1,160  of  11,044.    Elapsed: 0:08:42.\n",
      "  Batch 1,200  of  11,044.    Elapsed: 0:09:00.\n",
      "  Batch 1,240  of  11,044.    Elapsed: 0:09:18.\n",
      "  Batch 1,280  of  11,044.    Elapsed: 0:09:36.\n",
      "  Batch 1,320  of  11,044.    Elapsed: 0:09:54.\n",
      "  Batch 1,360  of  11,044.    Elapsed: 0:10:12.\n",
      "  Batch 1,400  of  11,044.    Elapsed: 0:10:30.\n",
      "  Batch 1,440  of  11,044.    Elapsed: 0:10:48.\n",
      "  Batch 1,480  of  11,044.    Elapsed: 0:11:06.\n",
      "  Batch 1,520  of  11,044.    Elapsed: 0:11:24.\n",
      "  Batch 1,560  of  11,044.    Elapsed: 0:11:42.\n",
      "  Batch 1,600  of  11,044.    Elapsed: 0:12:00.\n",
      "  Batch 1,640  of  11,044.    Elapsed: 0:12:18.\n",
      "  Batch 1,680  of  11,044.    Elapsed: 0:12:36.\n",
      "  Batch 1,720  of  11,044.    Elapsed: 0:12:54.\n",
      "  Batch 1,760  of  11,044.    Elapsed: 0:13:12.\n",
      "  Batch 1,800  of  11,044.    Elapsed: 0:13:30.\n",
      "  Batch 1,840  of  11,044.    Elapsed: 0:13:48.\n",
      "  Batch 1,880  of  11,044.    Elapsed: 0:14:06.\n",
      "  Batch 1,920  of  11,044.    Elapsed: 0:14:24.\n",
      "  Batch 1,960  of  11,044.    Elapsed: 0:14:42.\n",
      "  Batch 2,000  of  11,044.    Elapsed: 0:15:00.\n",
      "  Batch 2,040  of  11,044.    Elapsed: 0:15:18.\n",
      "  Batch 2,080  of  11,044.    Elapsed: 0:15:36.\n",
      "  Batch 2,120  of  11,044.    Elapsed: 0:15:54.\n",
      "  Batch 2,160  of  11,044.    Elapsed: 0:16:12.\n",
      "  Batch 2,200  of  11,044.    Elapsed: 0:16:30.\n",
      "  Batch 2,240  of  11,044.    Elapsed: 0:16:48.\n",
      "  Batch 2,280  of  11,044.    Elapsed: 0:17:06.\n",
      "  Batch 2,320  of  11,044.    Elapsed: 0:17:24.\n",
      "  Batch 2,360  of  11,044.    Elapsed: 0:17:42.\n",
      "  Batch 2,400  of  11,044.    Elapsed: 0:18:00.\n",
      "  Batch 2,440  of  11,044.    Elapsed: 0:18:18.\n",
      "  Batch 2,480  of  11,044.    Elapsed: 0:18:36.\n",
      "  Batch 2,520  of  11,044.    Elapsed: 0:18:54.\n",
      "  Batch 2,560  of  11,044.    Elapsed: 0:19:12.\n",
      "  Batch 2,600  of  11,044.    Elapsed: 0:19:30.\n",
      "  Batch 2,640  of  11,044.    Elapsed: 0:19:48.\n",
      "  Batch 2,680  of  11,044.    Elapsed: 0:20:06.\n",
      "  Batch 2,720  of  11,044.    Elapsed: 0:20:24.\n",
      "  Batch 2,760  of  11,044.    Elapsed: 0:20:42.\n",
      "  Batch 2,800  of  11,044.    Elapsed: 0:21:00.\n",
      "  Batch 2,840  of  11,044.    Elapsed: 0:21:18.\n",
      "  Batch 2,880  of  11,044.    Elapsed: 0:21:36.\n",
      "  Batch 2,920  of  11,044.    Elapsed: 0:21:54.\n",
      "  Batch 2,960  of  11,044.    Elapsed: 0:22:12.\n",
      "  Batch 3,000  of  11,044.    Elapsed: 0:22:30.\n",
      "  Batch 3,040  of  11,044.    Elapsed: 0:22:48.\n",
      "  Batch 3,080  of  11,044.    Elapsed: 0:23:06.\n",
      "  Batch 3,120  of  11,044.    Elapsed: 0:23:24.\n",
      "  Batch 3,160  of  11,044.    Elapsed: 0:23:42.\n",
      "  Batch 3,200  of  11,044.    Elapsed: 0:24:00.\n",
      "  Batch 3,240  of  11,044.    Elapsed: 0:24:18.\n",
      "  Batch 3,280  of  11,044.    Elapsed: 0:24:36.\n",
      "  Batch 3,320  of  11,044.    Elapsed: 0:24:54.\n",
      "  Batch 3,360  of  11,044.    Elapsed: 0:25:12.\n",
      "  Batch 3,400  of  11,044.    Elapsed: 0:25:30.\n",
      "  Batch 3,440  of  11,044.    Elapsed: 0:25:48.\n",
      "  Batch 3,480  of  11,044.    Elapsed: 0:26:06.\n",
      "  Batch 3,520  of  11,044.    Elapsed: 0:26:24.\n",
      "  Batch 3,560  of  11,044.    Elapsed: 0:26:42.\n",
      "  Batch 3,600  of  11,044.    Elapsed: 0:27:00.\n",
      "  Batch 3,640  of  11,044.    Elapsed: 0:27:18.\n",
      "  Batch 3,680  of  11,044.    Elapsed: 0:27:36.\n",
      "  Batch 3,720  of  11,044.    Elapsed: 0:27:54.\n",
      "  Batch 3,760  of  11,044.    Elapsed: 0:28:12.\n",
      "  Batch 3,800  of  11,044.    Elapsed: 0:28:30.\n",
      "  Batch 3,840  of  11,044.    Elapsed: 0:28:48.\n",
      "  Batch 3,880  of  11,044.    Elapsed: 0:29:06.\n",
      "  Batch 3,920  of  11,044.    Elapsed: 0:29:24.\n",
      "  Batch 3,960  of  11,044.    Elapsed: 0:29:42.\n",
      "  Batch 4,000  of  11,044.    Elapsed: 0:30:00.\n",
      "  Batch 4,040  of  11,044.    Elapsed: 0:30:18.\n",
      "  Batch 4,080  of  11,044.    Elapsed: 0:30:36.\n",
      "  Batch 4,120  of  11,044.    Elapsed: 0:30:54.\n",
      "  Batch 4,160  of  11,044.    Elapsed: 0:31:12.\n",
      "  Batch 4,200  of  11,044.    Elapsed: 0:31:30.\n",
      "  Batch 4,240  of  11,044.    Elapsed: 0:31:48.\n",
      "  Batch 4,280  of  11,044.    Elapsed: 0:32:06.\n",
      "  Batch 4,320  of  11,044.    Elapsed: 0:32:24.\n",
      "  Batch 4,360  of  11,044.    Elapsed: 0:32:42.\n",
      "  Batch 4,400  of  11,044.    Elapsed: 0:33:00.\n",
      "  Batch 4,440  of  11,044.    Elapsed: 0:33:18.\n",
      "  Batch 4,480  of  11,044.    Elapsed: 0:33:36.\n",
      "  Batch 4,520  of  11,044.    Elapsed: 0:33:54.\n",
      "  Batch 4,560  of  11,044.    Elapsed: 0:34:12.\n",
      "  Batch 4,600  of  11,044.    Elapsed: 0:34:30.\n",
      "  Batch 4,640  of  11,044.    Elapsed: 0:34:48.\n",
      "  Batch 4,680  of  11,044.    Elapsed: 0:35:06.\n",
      "  Batch 4,720  of  11,044.    Elapsed: 0:35:24.\n",
      "  Batch 4,760  of  11,044.    Elapsed: 0:35:42.\n",
      "  Batch 4,800  of  11,044.    Elapsed: 0:36:00.\n",
      "  Batch 4,840  of  11,044.    Elapsed: 0:36:18.\n",
      "  Batch 4,880  of  11,044.    Elapsed: 0:36:36.\n",
      "  Batch 4,920  of  11,044.    Elapsed: 0:36:54.\n",
      "  Batch 4,960  of  11,044.    Elapsed: 0:37:12.\n",
      "  Batch 5,000  of  11,044.    Elapsed: 0:37:30.\n",
      "  Batch 5,040  of  11,044.    Elapsed: 0:37:48.\n",
      "  Batch 5,080  of  11,044.    Elapsed: 0:38:06.\n",
      "  Batch 5,120  of  11,044.    Elapsed: 0:38:24.\n",
      "  Batch 5,160  of  11,044.    Elapsed: 0:38:42.\n",
      "  Batch 5,200  of  11,044.    Elapsed: 0:39:00.\n",
      "  Batch 5,240  of  11,044.    Elapsed: 0:39:18.\n",
      "  Batch 5,280  of  11,044.    Elapsed: 0:39:36.\n",
      "  Batch 5,320  of  11,044.    Elapsed: 0:39:54.\n",
      "  Batch 5,360  of  11,044.    Elapsed: 0:40:12.\n",
      "  Batch 5,400  of  11,044.    Elapsed: 0:40:30.\n",
      "  Batch 5,440  of  11,044.    Elapsed: 0:40:48.\n",
      "  Batch 5,480  of  11,044.    Elapsed: 0:41:06.\n",
      "  Batch 5,520  of  11,044.    Elapsed: 0:41:24.\n",
      "  Batch 5,560  of  11,044.    Elapsed: 0:41:42.\n",
      "  Batch 5,600  of  11,044.    Elapsed: 0:42:00.\n",
      "  Batch 5,640  of  11,044.    Elapsed: 0:42:18.\n",
      "  Batch 5,680  of  11,044.    Elapsed: 0:42:36.\n",
      "  Batch 5,720  of  11,044.    Elapsed: 0:42:54.\n",
      "  Batch 5,760  of  11,044.    Elapsed: 0:43:12.\n",
      "  Batch 5,800  of  11,044.    Elapsed: 0:43:30.\n",
      "  Batch 5,840  of  11,044.    Elapsed: 0:43:48.\n",
      "  Batch 5,880  of  11,044.    Elapsed: 0:44:06.\n",
      "  Batch 5,920  of  11,044.    Elapsed: 0:44:24.\n",
      "  Batch 5,960  of  11,044.    Elapsed: 0:44:42.\n",
      "  Batch 6,000  of  11,044.    Elapsed: 0:45:00.\n",
      "  Batch 6,040  of  11,044.    Elapsed: 0:45:18.\n",
      "  Batch 6,080  of  11,044.    Elapsed: 0:45:36.\n",
      "  Batch 6,120  of  11,044.    Elapsed: 0:45:54.\n",
      "  Batch 6,160  of  11,044.    Elapsed: 0:46:12.\n",
      "  Batch 6,200  of  11,044.    Elapsed: 0:46:30.\n",
      "  Batch 6,240  of  11,044.    Elapsed: 0:46:48.\n",
      "  Batch 6,280  of  11,044.    Elapsed: 0:47:06.\n",
      "  Batch 6,320  of  11,044.    Elapsed: 0:47:24.\n",
      "  Batch 6,360  of  11,044.    Elapsed: 0:47:42.\n",
      "  Batch 6,400  of  11,044.    Elapsed: 0:48:00.\n",
      "  Batch 6,440  of  11,044.    Elapsed: 0:48:18.\n",
      "  Batch 6,480  of  11,044.    Elapsed: 0:48:36.\n",
      "  Batch 6,520  of  11,044.    Elapsed: 0:48:54.\n",
      "  Batch 6,560  of  11,044.    Elapsed: 0:49:12.\n",
      "  Batch 6,600  of  11,044.    Elapsed: 0:49:30.\n",
      "  Batch 6,640  of  11,044.    Elapsed: 0:49:48.\n",
      "  Batch 6,680  of  11,044.    Elapsed: 0:50:06.\n",
      "  Batch 6,720  of  11,044.    Elapsed: 0:50:24.\n",
      "  Batch 6,760  of  11,044.    Elapsed: 0:50:42.\n",
      "  Batch 6,800  of  11,044.    Elapsed: 0:51:00.\n",
      "  Batch 6,840  of  11,044.    Elapsed: 0:51:18.\n",
      "  Batch 6,880  of  11,044.    Elapsed: 0:51:36.\n",
      "  Batch 6,920  of  11,044.    Elapsed: 0:51:54.\n",
      "  Batch 6,960  of  11,044.    Elapsed: 0:52:12.\n",
      "  Batch 7,000  of  11,044.    Elapsed: 0:52:30.\n",
      "  Batch 7,040  of  11,044.    Elapsed: 0:52:48.\n",
      "  Batch 7,080  of  11,044.    Elapsed: 0:53:06.\n",
      "  Batch 7,120  of  11,044.    Elapsed: 0:53:24.\n",
      "  Batch 7,160  of  11,044.    Elapsed: 0:53:42.\n",
      "  Batch 7,200  of  11,044.    Elapsed: 0:54:00.\n",
      "  Batch 7,240  of  11,044.    Elapsed: 0:54:18.\n",
      "  Batch 7,280  of  11,044.    Elapsed: 0:54:36.\n",
      "  Batch 7,320  of  11,044.    Elapsed: 0:54:54.\n",
      "  Batch 7,360  of  11,044.    Elapsed: 0:55:12.\n",
      "  Batch 7,400  of  11,044.    Elapsed: 0:55:29.\n",
      "  Batch 7,440  of  11,044.    Elapsed: 0:55:47.\n",
      "  Batch 7,480  of  11,044.    Elapsed: 0:56:05.\n",
      "  Batch 7,520  of  11,044.    Elapsed: 0:56:23.\n",
      "  Batch 7,560  of  11,044.    Elapsed: 0:56:41.\n",
      "  Batch 7,600  of  11,044.    Elapsed: 0:56:59.\n",
      "  Batch 7,640  of  11,044.    Elapsed: 0:57:17.\n",
      "  Batch 7,680  of  11,044.    Elapsed: 0:57:35.\n",
      "  Batch 7,720  of  11,044.    Elapsed: 0:57:53.\n",
      "  Batch 7,760  of  11,044.    Elapsed: 0:58:11.\n",
      "  Batch 7,800  of  11,044.    Elapsed: 0:58:29.\n",
      "  Batch 7,840  of  11,044.    Elapsed: 0:58:47.\n",
      "  Batch 7,880  of  11,044.    Elapsed: 0:59:05.\n",
      "  Batch 7,920  of  11,044.    Elapsed: 0:59:23.\n",
      "  Batch 7,960  of  11,044.    Elapsed: 0:59:41.\n",
      "  Batch 8,000  of  11,044.    Elapsed: 0:59:59.\n",
      "  Batch 8,040  of  11,044.    Elapsed: 1:00:17.\n",
      "  Batch 8,080  of  11,044.    Elapsed: 1:00:35.\n",
      "  Batch 8,120  of  11,044.    Elapsed: 1:00:53.\n",
      "  Batch 8,160  of  11,044.    Elapsed: 1:01:11.\n",
      "  Batch 8,200  of  11,044.    Elapsed: 1:01:29.\n",
      "  Batch 8,240  of  11,044.    Elapsed: 1:01:47.\n",
      "  Batch 8,280  of  11,044.    Elapsed: 1:02:05.\n",
      "  Batch 8,320  of  11,044.    Elapsed: 1:02:23.\n",
      "  Batch 8,360  of  11,044.    Elapsed: 1:02:41.\n",
      "  Batch 8,400  of  11,044.    Elapsed: 1:02:59.\n",
      "  Batch 8,440  of  11,044.    Elapsed: 1:03:17.\n",
      "  Batch 8,480  of  11,044.    Elapsed: 1:03:35.\n",
      "  Batch 8,520  of  11,044.    Elapsed: 1:03:53.\n",
      "  Batch 8,560  of  11,044.    Elapsed: 1:04:11.\n",
      "  Batch 8,600  of  11,044.    Elapsed: 1:04:29.\n",
      "  Batch 8,640  of  11,044.    Elapsed: 1:04:47.\n",
      "  Batch 8,680  of  11,044.    Elapsed: 1:05:05.\n",
      "  Batch 8,720  of  11,044.    Elapsed: 1:05:23.\n",
      "  Batch 8,760  of  11,044.    Elapsed: 1:05:41.\n",
      "  Batch 8,800  of  11,044.    Elapsed: 1:05:59.\n",
      "  Batch 8,840  of  11,044.    Elapsed: 1:06:17.\n",
      "  Batch 8,880  of  11,044.    Elapsed: 1:06:35.\n",
      "  Batch 8,920  of  11,044.    Elapsed: 1:06:53.\n",
      "  Batch 8,960  of  11,044.    Elapsed: 1:07:11.\n",
      "  Batch 9,000  of  11,044.    Elapsed: 1:07:29.\n",
      "  Batch 9,040  of  11,044.    Elapsed: 1:07:47.\n",
      "  Batch 9,080  of  11,044.    Elapsed: 1:08:05.\n",
      "  Batch 9,120  of  11,044.    Elapsed: 1:08:23.\n",
      "  Batch 9,160  of  11,044.    Elapsed: 1:08:41.\n",
      "  Batch 9,200  of  11,044.    Elapsed: 1:08:59.\n",
      "  Batch 9,240  of  11,044.    Elapsed: 1:09:17.\n",
      "  Batch 9,280  of  11,044.    Elapsed: 1:09:35.\n",
      "  Batch 9,320  of  11,044.    Elapsed: 1:09:53.\n",
      "  Batch 9,360  of  11,044.    Elapsed: 1:10:11.\n",
      "  Batch 9,400  of  11,044.    Elapsed: 1:10:29.\n",
      "  Batch 9,440  of  11,044.    Elapsed: 1:10:47.\n",
      "  Batch 9,480  of  11,044.    Elapsed: 1:11:05.\n",
      "  Batch 9,520  of  11,044.    Elapsed: 1:11:23.\n",
      "  Batch 9,560  of  11,044.    Elapsed: 1:11:41.\n",
      "  Batch 9,600  of  11,044.    Elapsed: 1:11:59.\n",
      "  Batch 9,640  of  11,044.    Elapsed: 1:12:17.\n",
      "  Batch 9,680  of  11,044.    Elapsed: 1:12:35.\n",
      "  Batch 9,720  of  11,044.    Elapsed: 1:12:53.\n",
      "  Batch 9,760  of  11,044.    Elapsed: 1:13:11.\n",
      "  Batch 9,800  of  11,044.    Elapsed: 1:13:29.\n",
      "  Batch 9,840  of  11,044.    Elapsed: 1:13:47.\n",
      "  Batch 9,880  of  11,044.    Elapsed: 1:14:05.\n",
      "  Batch 9,920  of  11,044.    Elapsed: 1:14:23.\n",
      "  Batch 9,960  of  11,044.    Elapsed: 1:14:41.\n",
      "  Batch 10,000  of  11,044.    Elapsed: 1:14:59.\n",
      "  Batch 10,040  of  11,044.    Elapsed: 1:15:17.\n",
      "  Batch 10,080  of  11,044.    Elapsed: 1:15:35.\n",
      "  Batch 10,120  of  11,044.    Elapsed: 1:15:53.\n",
      "  Batch 10,160  of  11,044.    Elapsed: 1:16:11.\n",
      "  Batch 10,200  of  11,044.    Elapsed: 1:16:29.\n",
      "  Batch 10,240  of  11,044.    Elapsed: 1:16:47.\n",
      "  Batch 10,280  of  11,044.    Elapsed: 1:17:05.\n",
      "  Batch 10,320  of  11,044.    Elapsed: 1:17:23.\n",
      "  Batch 10,360  of  11,044.    Elapsed: 1:17:41.\n",
      "  Batch 10,400  of  11,044.    Elapsed: 1:17:59.\n",
      "  Batch 10,440  of  11,044.    Elapsed: 1:18:17.\n",
      "  Batch 10,480  of  11,044.    Elapsed: 1:18:35.\n",
      "  Batch 10,520  of  11,044.    Elapsed: 1:18:53.\n",
      "  Batch 10,560  of  11,044.    Elapsed: 1:19:11.\n",
      "  Batch 10,600  of  11,044.    Elapsed: 1:19:29.\n",
      "  Batch 10,640  of  11,044.    Elapsed: 1:19:47.\n",
      "  Batch 10,680  of  11,044.    Elapsed: 1:20:05.\n",
      "  Batch 10,720  of  11,044.    Elapsed: 1:20:23.\n",
      "  Batch 10,760  of  11,044.    Elapsed: 1:20:41.\n",
      "  Batch 10,800  of  11,044.    Elapsed: 1:20:59.\n",
      "  Batch 10,840  of  11,044.    Elapsed: 1:21:17.\n",
      "  Batch 10,880  of  11,044.    Elapsed: 1:21:35.\n",
      "  Batch 10,920  of  11,044.    Elapsed: 1:21:53.\n",
      "  Batch 10,960  of  11,044.    Elapsed: 1:22:11.\n",
      "  Batch 11,000  of  11,044.    Elapsed: 1:22:29.\n",
      "  Batch 11,040  of  11,044.    Elapsed: 1:22:47.\n",
      "\n",
      "  Average training loss: 0.54\n",
      "  Training epoch took: 1:22:49\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.83\n",
      "  Validation took: 0:02:42\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of  11,044.    Elapsed: 0:00:18.\n",
      "  Batch    80  of  11,044.    Elapsed: 0:00:36.\n",
      "  Batch   120  of  11,044.    Elapsed: 0:00:54.\n",
      "  Batch   160  of  11,044.    Elapsed: 0:01:11.\n",
      "  Batch   200  of  11,044.    Elapsed: 0:01:29.\n",
      "  Batch   240  of  11,044.    Elapsed: 0:01:47.\n",
      "  Batch   280  of  11,044.    Elapsed: 0:02:05.\n",
      "  Batch   320  of  11,044.    Elapsed: 0:02:23.\n",
      "  Batch   360  of  11,044.    Elapsed: 0:02:41.\n",
      "  Batch   400  of  11,044.    Elapsed: 0:02:59.\n",
      "  Batch   440  of  11,044.    Elapsed: 0:03:17.\n",
      "  Batch   480  of  11,044.    Elapsed: 0:03:34.\n",
      "  Batch   520  of  11,044.    Elapsed: 0:03:52.\n",
      "  Batch   560  of  11,044.    Elapsed: 0:04:10.\n",
      "  Batch   600  of  11,044.    Elapsed: 0:04:28.\n",
      "  Batch   640  of  11,044.    Elapsed: 0:04:46.\n",
      "  Batch   680  of  11,044.    Elapsed: 0:05:04.\n",
      "  Batch   720  of  11,044.    Elapsed: 0:05:22.\n",
      "  Batch   760  of  11,044.    Elapsed: 0:05:39.\n",
      "  Batch   800  of  11,044.    Elapsed: 0:05:57.\n",
      "  Batch   840  of  11,044.    Elapsed: 0:06:15.\n",
      "  Batch   880  of  11,044.    Elapsed: 0:06:33.\n",
      "  Batch   920  of  11,044.    Elapsed: 0:06:51.\n",
      "  Batch   960  of  11,044.    Elapsed: 0:07:09.\n",
      "  Batch 1,000  of  11,044.    Elapsed: 0:07:27.\n",
      "  Batch 1,040  of  11,044.    Elapsed: 0:07:45.\n",
      "  Batch 1,080  of  11,044.    Elapsed: 0:08:02.\n",
      "  Batch 1,120  of  11,044.    Elapsed: 0:08:20.\n",
      "  Batch 1,160  of  11,044.    Elapsed: 0:08:38.\n",
      "  Batch 1,200  of  11,044.    Elapsed: 0:08:56.\n",
      "  Batch 1,240  of  11,044.    Elapsed: 0:09:14.\n",
      "  Batch 1,280  of  11,044.    Elapsed: 0:09:32.\n",
      "  Batch 1,320  of  11,044.    Elapsed: 0:09:50.\n",
      "  Batch 1,360  of  11,044.    Elapsed: 0:10:08.\n",
      "  Batch 1,400  of  11,044.    Elapsed: 0:10:26.\n",
      "  Batch 1,440  of  11,044.    Elapsed: 0:10:43.\n",
      "  Batch 1,480  of  11,044.    Elapsed: 0:11:01.\n",
      "  Batch 1,520  of  11,044.    Elapsed: 0:11:19.\n",
      "  Batch 1,560  of  11,044.    Elapsed: 0:11:37.\n",
      "  Batch 1,600  of  11,044.    Elapsed: 0:11:55.\n",
      "  Batch 1,640  of  11,044.    Elapsed: 0:12:13.\n",
      "  Batch 1,680  of  11,044.    Elapsed: 0:12:31.\n",
      "  Batch 1,720  of  11,044.    Elapsed: 0:12:49.\n",
      "  Batch 1,760  of  11,044.    Elapsed: 0:13:07.\n",
      "  Batch 1,800  of  11,044.    Elapsed: 0:13:25.\n",
      "  Batch 1,840  of  11,044.    Elapsed: 0:13:42.\n",
      "  Batch 1,880  of  11,044.    Elapsed: 0:14:00.\n",
      "  Batch 1,920  of  11,044.    Elapsed: 0:14:18.\n",
      "  Batch 1,960  of  11,044.    Elapsed: 0:14:36.\n",
      "  Batch 2,000  of  11,044.    Elapsed: 0:14:54.\n",
      "  Batch 2,040  of  11,044.    Elapsed: 0:15:12.\n",
      "  Batch 2,080  of  11,044.    Elapsed: 0:15:30.\n",
      "  Batch 2,120  of  11,044.    Elapsed: 0:15:48.\n",
      "  Batch 2,160  of  11,044.    Elapsed: 0:16:06.\n",
      "  Batch 2,200  of  11,044.    Elapsed: 0:16:23.\n",
      "  Batch 2,240  of  11,044.    Elapsed: 0:16:41.\n",
      "  Batch 2,280  of  11,044.    Elapsed: 0:16:59.\n",
      "  Batch 2,320  of  11,044.    Elapsed: 0:17:17.\n",
      "  Batch 2,360  of  11,044.    Elapsed: 0:17:35.\n",
      "  Batch 2,400  of  11,044.    Elapsed: 0:17:53.\n",
      "  Batch 2,440  of  11,044.    Elapsed: 0:18:11.\n",
      "  Batch 2,480  of  11,044.    Elapsed: 0:18:29.\n",
      "  Batch 2,520  of  11,044.    Elapsed: 0:18:47.\n",
      "  Batch 2,560  of  11,044.    Elapsed: 0:19:05.\n",
      "  Batch 2,600  of  11,044.    Elapsed: 0:19:23.\n",
      "  Batch 2,640  of  11,044.    Elapsed: 0:19:40.\n",
      "  Batch 2,680  of  11,044.    Elapsed: 0:19:58.\n",
      "  Batch 2,720  of  11,044.    Elapsed: 0:20:16.\n",
      "  Batch 2,760  of  11,044.    Elapsed: 0:20:34.\n",
      "  Batch 2,800  of  11,044.    Elapsed: 0:20:52.\n",
      "  Batch 2,840  of  11,044.    Elapsed: 0:21:10.\n",
      "  Batch 2,880  of  11,044.    Elapsed: 0:21:28.\n",
      "  Batch 2,920  of  11,044.    Elapsed: 0:21:46.\n",
      "  Batch 2,960  of  11,044.    Elapsed: 0:22:04.\n",
      "  Batch 3,000  of  11,044.    Elapsed: 0:22:22.\n",
      "  Batch 3,040  of  11,044.    Elapsed: 0:22:39.\n",
      "  Batch 3,080  of  11,044.    Elapsed: 0:22:57.\n",
      "  Batch 3,120  of  11,044.    Elapsed: 0:23:15.\n",
      "  Batch 3,160  of  11,044.    Elapsed: 0:23:33.\n",
      "  Batch 3,200  of  11,044.    Elapsed: 0:23:51.\n",
      "  Batch 3,240  of  11,044.    Elapsed: 0:24:09.\n",
      "  Batch 3,280  of  11,044.    Elapsed: 0:24:27.\n",
      "  Batch 3,320  of  11,044.    Elapsed: 0:24:45.\n",
      "  Batch 3,360  of  11,044.    Elapsed: 0:25:03.\n",
      "  Batch 3,400  of  11,044.    Elapsed: 0:25:21.\n",
      "  Batch 3,440  of  11,044.    Elapsed: 0:25:39.\n",
      "  Batch 3,480  of  11,044.    Elapsed: 0:25:56.\n",
      "  Batch 3,520  of  11,044.    Elapsed: 0:26:14.\n",
      "  Batch 3,560  of  11,044.    Elapsed: 0:26:32.\n",
      "  Batch 3,600  of  11,044.    Elapsed: 0:26:50.\n",
      "  Batch 3,640  of  11,044.    Elapsed: 0:27:08.\n",
      "  Batch 3,680  of  11,044.    Elapsed: 0:27:26.\n",
      "  Batch 3,720  of  11,044.    Elapsed: 0:27:44.\n",
      "  Batch 3,760  of  11,044.    Elapsed: 0:28:02.\n",
      "  Batch 3,800  of  11,044.    Elapsed: 0:28:20.\n",
      "  Batch 3,840  of  11,044.    Elapsed: 0:28:38.\n",
      "  Batch 3,880  of  11,044.    Elapsed: 0:28:56.\n",
      "  Batch 3,920  of  11,044.    Elapsed: 0:29:14.\n",
      "  Batch 3,960  of  11,044.    Elapsed: 0:29:31.\n",
      "  Batch 4,000  of  11,044.    Elapsed: 0:29:49.\n",
      "  Batch 4,040  of  11,044.    Elapsed: 0:30:07.\n",
      "  Batch 4,080  of  11,044.    Elapsed: 0:30:25.\n",
      "  Batch 4,120  of  11,044.    Elapsed: 0:30:43.\n",
      "  Batch 4,160  of  11,044.    Elapsed: 0:31:01.\n",
      "  Batch 4,200  of  11,044.    Elapsed: 0:31:19.\n",
      "  Batch 4,240  of  11,044.    Elapsed: 0:31:37.\n",
      "  Batch 4,280  of  11,044.    Elapsed: 0:31:55.\n",
      "  Batch 4,320  of  11,044.    Elapsed: 0:32:13.\n",
      "  Batch 4,360  of  11,044.    Elapsed: 0:32:31.\n",
      "  Batch 4,400  of  11,044.    Elapsed: 0:32:49.\n",
      "  Batch 4,440  of  11,044.    Elapsed: 0:33:07.\n",
      "  Batch 4,480  of  11,044.    Elapsed: 0:33:24.\n",
      "  Batch 4,520  of  11,044.    Elapsed: 0:33:42.\n",
      "  Batch 4,560  of  11,044.    Elapsed: 0:34:00.\n",
      "  Batch 4,600  of  11,044.    Elapsed: 0:34:18.\n",
      "  Batch 4,640  of  11,044.    Elapsed: 0:34:36.\n",
      "  Batch 4,680  of  11,044.    Elapsed: 0:34:54.\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_loss = 0 #Reset loss value for the next epoch\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:        # Progress update every 40 batches.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad() # Clear previously calculated gradients     \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader) # Calculate the average loss over the training data.           \n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKRzSMHgSzfU"
   },
   "outputs": [],
   "source": [
    "#Plot val loss\n",
    "sns.set(style='darkgrid')\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "plt.plot(loss_values, 'b-o')\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CA0Rjrg7SzdY"
   },
   "outputs": [],
   "source": [
    "### Save the model\n",
    "model.save_pretrained('C:\\\\Users\\\\jujub\\\\Documents\\\\DeepLearnning Project\\\\my_saved_model_directory' )\n",
    "tokenizer.save_pretrained('C:\\\\Users\\\\jujub\\\\Documents\\\\my_saved_model_directory\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fX8bBdsSzbn"
   },
   "outputs": [],
   "source": [
    "### Reload the model and the tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./my_saved_model_directory/')\n",
    "tokenizer = BertTokenizer.from_pretrained('./my_saved_model_directory/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uy_byquMDa2K"
   },
   "source": [
    "## Prediction of the labels for the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6CplY8NSzRv"
   },
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences_1 = df_test.sentence_1.values\n",
    "sentences_2 = df_test.sentence_2.values\n",
    "\n",
    "input_ids_1 = []\n",
    "\n",
    "for sent in sentences_1:\n",
    "\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_LEN,\n",
    "                   )\n",
    "    input_ids_1.append(encoded_sent)\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids_1 = pad_sequences(input_ids_1, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Same for the second sentence\n",
    "input_ids_2 = []\n",
    "\n",
    "for sent in sentences_2:\n",
    "\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_LEN,\n",
    "                   )\n",
    "    input_ids_2.append(encoded_sent)\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids_2 = pad_sequences(input_ids_2, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Concatenate the two padded tokenized sentence to have only one input\n",
    "input_ids = np.concatenate((input_ids_1, input_ids_2), axis=1)\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tADKCd7FSzO2"
   },
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "  b_input_ids, b_input_mask = batch\n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "  logits = outputs[0]\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "\n",
    "  predictions.append(logits)\n",
    "\n",
    "print('    DONE.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5NX-7I_ZXcTA"
   },
   "outputs": [],
   "source": [
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "print(flat_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5eHaYOPXgdt"
   },
   "outputs": [],
   "source": [
    "# Take back the real label \n",
    "flat_predictions = label_encoder.inverse_transform(flat_predictions)\n",
    "print(flat_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnwat8hKYU2D"
   },
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame({'index': test_X.index, 'label': flat_predictions})\n",
    "my_submission.to_csv('submission.csv', index=False)\n",
    "print(my_submission.shape)\n",
    "my_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5b7FPv7nuu3"
   },
   "outputs": [],
   "source": [
    "files.download('submission.csv')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_fine_tunes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
